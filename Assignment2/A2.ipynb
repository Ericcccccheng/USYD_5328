{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e5557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import numpy as np, random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca8ca69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(NpzFile 'datasets/FashionMNIST0.6.npz' with keys: Xtr, Str, Xts, Yts)\n",
      "Training data shape: (18000, 784)\n",
      "Training labels shape: (18000,)\n",
      "Test data shape: (3000, 784)\n",
      "Test labels shape: (3000,)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = 'datasets/FashionMNIST0.3.npz'\n",
    "dataset2 = 'datasets/FashionMNIST0.6.npz'\n",
    "dataset3 = 'datasets/CIFAR.npz'\n",
    "\n",
    "data = np.load(dataset2)\n",
    "\n",
    "print(data.keys())\n",
    "\n",
    "Xtr = data['Xtr']\n",
    "Str = data['Str']\n",
    "Xts = data['Xts']\n",
    "Yts = data['Yts']\n",
    "\n",
    "print(\"Training data shape:\", Xtr.shape)\n",
    "print(\"Training labels shape:\", Str.shape)\n",
    "print(\"Test data shape:\", Xts.shape)\n",
    "print(\"Test labels shape:\", Yts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_T_by_repeats(X, s, n_classes=3, R=10, seed=0):\n",
    "    T_counts = np.zeros((n_classes, n_classes), dtype=float)\n",
    "    for r in range(R):\n",
    "        X_tr, X_va, s_tr, s_va = train_test_split(\n",
    "            X, s, test_size=0.2, stratify=s, random_state=seed + r\n",
    "        )\n",
    "        clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=seed + r)\n",
    "        y_hat = clf.fit(X_tr, s_tr).predict(X_va)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            mask = (y_hat == i)\n",
    "            if mask.sum() == 0:\n",
    "                T_counts[i] += 1.0\n",
    "            else:\n",
    "                T_counts[i] += np.bincount(s_va[mask], minlength=n_classes)\n",
    "\n",
    "    T_counts += 1e-6\n",
    "    T_hat = T_counts / T_counts.sum(axis=1, keepdims=True)\n",
    "    return T_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f00b68a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7 0.3 0. ]\n",
      " [0.  0.7 0.3]\n",
      " [0.3 0.  0.7]]\n"
     ]
    }
   ],
   "source": [
    "data = np.load(dataset1)\n",
    "X = data['Xtr'].reshape(len(data['Xtr']), -1)\n",
    "s = data['Str']\n",
    "\n",
    "seed = 10086\n",
    "\n",
    "T_hat = estimate_T_by_repeats(X, s, n_classes=3, R=10, seed=seed)\n",
    "print(np.round(T_hat, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43aace3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4 0.3 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n"
     ]
    }
   ],
   "source": [
    "data2 = np.load(dataset2)\n",
    "X = data2['Xtr'].reshape(len(data2['Xtr']), -1)\n",
    "s = data2['Str']\n",
    "\n",
    "seed = 10086\n",
    "\n",
    "T_hat = estimate_T_by_repeats(X, s, n_classes=3, R=10, seed=seed)\n",
    "print(np.round(T_hat, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69e3f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4 0.3 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n"
     ]
    }
   ],
   "source": [
    "data3 = np.load(dataset3)\n",
    "X = data3['Xtr'].reshape(len(data3\n",
    "                             ['Xtr']), -1)\n",
    "s = data3['Str']\n",
    "\n",
    "seed = 10086\n",
    "\n",
    "T_hat = estimate_T_by_repeats(X, s, n_classes=3, R=10, seed=seed)\n",
    "print(np.round(T_hat, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96b564f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.3 0.2]\n",
      " [0.3 0.5 0.2]\n",
      " [0.3 0.3 0.5]]\n"
     ]
    }
   ],
   "source": [
    "data = np.load(dataset3)\n",
    "X_tr1 = data['Xtr']\n",
    "S_tr1 = data['Str']\n",
    "\n",
    "X_tr1_flatten = X_tr1.reshape((X_tr1.shape[0], -1))\n",
    "\n",
    "X_train, X_val, S_train, S_val = train_test_split(X_tr1_flatten, S_tr1, test_size=0.2, random_state=42)\n",
    "\n",
    "n_classes = 3\n",
    "T = np.zeros((n_classes, n_classes))\n",
    "\n",
    "seed = 10086\n",
    "np.random.seed(seed); random.seed(seed); os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "#model = LogisticRegression(max_iter=500) 0.6\n",
    "#model = RandomForestClassifier() 0.7   \n",
    "#model = make_pipeline(StandardScaler(),SVC()) 0.7\n",
    "model = ExtraTreesClassifier(n_estimators=500, random_state=seed, n_jobs=-1)\n",
    "X_train, X_val, S_train, S_val = train_test_split(\n",
    "    X_tr1_flatten, S_tr1, test_size=0.2, stratify=S_tr1, random_state=seed\n",
    ")\n",
    "\n",
    "pre_labels = model.fit(X_train, S_train).predict_proba(X_val)\n",
    "\n",
    "for i in range(3):\n",
    "    idx = np.argsort(pre_labels[:, i])[-max(1, int(0.03*len(S_val))):]\n",
    "    T[i] = np.mean(pre_labels[idx], axis=0)\n",
    "\n",
    "T = T / T.sum(axis=1, keepdims=True)\n",
    "\n",
    "print(np.round(T, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23238a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3 0.4 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n"
     ]
    }
   ],
   "source": [
    "data2 = np.load(dataset2)\n",
    "X_tr2 = data2['Xtr']\n",
    "S_tr2 = data2['Str']\n",
    "\n",
    "X_tr2_flatten = X_tr2.reshape((X_tr2.shape[0], -1))\n",
    "\n",
    "X_train, X_val, S_train, S_val = train_test_split(X_tr2_flatten, S_tr2, test_size=0.2, random_state=42)\n",
    "\n",
    "n_classes = 3\n",
    "T2 = np.zeros((n_classes, n_classes))\n",
    "\n",
    "#model = LogisticRegression(max_iter=500) 0.6\n",
    "#model = RandomForestClassifier() 0.7\n",
    "#model = make_pipeline(StandardScaler(),SVC()) 0.7\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train, S_train)\n",
    "\n",
    "pre_labels = model.predict(X_val)\n",
    "\n",
    "for i in range(len(S_val)):\n",
    "    noi_label = S_val[i]\n",
    "    pre_label = pre_labels[i]\n",
    "    T2[pre_label, noi_label] += 1\n",
    "\n",
    "T2 = T2 / T2.sum(axis=1, keepdims=True)\n",
    "\n",
    "T2_orig = T2.copy()\n",
    "T2 = np.round(T2, 1)\n",
    "\n",
    "row_sums = T2.sum(axis=1)\n",
    "rows_to_fix = np.where(np.isclose(row_sums, 0.9))[0]\n",
    "\n",
    "for r in rows_to_fix:\n",
    "    c = int(np.argmax(T2_orig[r]))\n",
    "    T2[r, c] = np.round(T2[r, c] + 0.1, 1)\n",
    "\n",
    "print(T2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e5b2c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3 0.4 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n"
     ]
    }
   ],
   "source": [
    "data3 = np.load(dataset3)\n",
    "X_tr3 = data3['Xtr']\n",
    "S_tr3 = data3['Str']\n",
    "\n",
    "X_tr3_flatten = X_tr3.reshape((X_tr3.shape[0], -1))\n",
    "\n",
    "X_train, X_val, S_train, S_val = train_test_split(X_tr3_flatten, S_tr3, test_size=0.2, random_state=42)\n",
    "\n",
    "n_classes = 3\n",
    "T3 = np.zeros((n_classes, n_classes))\n",
    "\n",
    "#model = LogisticRegression(max_iter=500) 0.6\n",
    "#model = RandomForestClassifier() 0.7\n",
    "#model = make_pipeline(StandardScaler(),SVC()) 0.7\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train, S_train)\n",
    "\n",
    "pre_labels = model.predict(X_val)\n",
    "\n",
    "for i in range(len(S_val)):\n",
    "    noi_label = S_val[i]\n",
    "    pre_label = pre_labels[i]\n",
    "    T3[pre_label, noi_label] += 1\n",
    "\n",
    "T3 = T3 / T3.sum(axis=1, keepdims=True)\n",
    "\n",
    "T3_orig = T3.copy()\n",
    "T3 = np.round(T3, 1)\n",
    "\n",
    "row_sums = T3.sum(axis=1)\n",
    "rows_to_fix = np.where(np.isclose(row_sums, 0.9))[0]\n",
    "\n",
    "for r in rows_to_fix:\n",
    "    c = int(np.argmax(T3_orig[r]))\n",
    "    T3[r, c] = np.round(T3[r, c] + 0.1, 1)\n",
    "\n",
    "print(T3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea345c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fee300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coteaching_correction_loss(T=None, total_epochs=10, Tk=10, default_tau=0.2):\n",
    "    # —— 从 T 估噪声率（可选）——\n",
    "    def _estimate_tau(Tmat, default_tau=0.2):\n",
    "        if Tmat is None:\n",
    "            return float(default_tau)\n",
    "        Tmat = np.asarray(Tmat, dtype=np.float32)\n",
    "        C = Tmat.shape[0]\n",
    "        return float(1.0 - np.trace(Tmat) / C)  # tau ≈ 1 - trace(T)/C\n",
    "\n",
    "    tau = _estimate_tau(T, default_tau=default_tau)\n",
    "    keep_rate = tf.Variable(1.0, trainable=False, dtype=tf.float32, name='keep_rate')\n",
    "\n",
    "    # —— 损失函数：只对小损失子集取平均 ——\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # y_pred 是 softmax 概率（非 logits）\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)\n",
    "        ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)  # [B]\n",
    "        B = tf.shape(ce)[0]\n",
    "        k = tf.maximum(1, tf.cast(tf.floor(keep_rate * tf.cast(B, tf.float32)), tf.int32))\n",
    "        idx = tf.argsort(ce)[:k]                       # 最小损失的前 k 个\n",
    "        ce_small = tf.gather(ce, idx)                  # 选中的小损失样本\n",
    "        return tf.reduce_mean(ce_small)\n",
    "\n",
    "    # —— 回调：每个 epoch 开始时更新 keep_rate —— \n",
    "    class _CoTeachSchedule(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, tau, Tk):\n",
    "            super().__init__()\n",
    "            self.tau = float(tau)\n",
    "            self.Tk  = int(Tk)\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            # epoch 从 0 开始，公式用 (epoch+1)\n",
    "            kr = 1.0 - min(((epoch + 1) / self.Tk) * self.tau, self.tau)\n",
    "            keep_rate.assign(kr)\n",
    "\n",
    "    schedule_cb = _CoTeachSchedule(tau, Tk)\n",
    "    return loss_fn, schedule_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== C) 仅 Co-teaching =====================\n",
    "results_coteach = []  # (dataset, 'Co-teaching', mean, std)\n",
    "\n",
    "# 直接用你现在的 datasets（字典）+ T_by_dataset_est（字典）\n",
    "for name, path in datasets.items():\n",
    "    T_used = T_by_dataset_est[name]  # 与数据集同名键的一一对应矩阵\n",
    "    mean_c, std_c, _ = run_model_on_dataset(\n",
    "        path, T_used, model_kind='co-teaching',\n",
    "        num_runs=NUM_RUNS, seed=SEED,\n",
    "        epochs=EPOCHS, batch_size=BATCH, verbose=0\n",
    "    )\n",
    "    results_coteach.append((name, 'Co-teaching', mean_c, std_c))\n",
    "\n",
    "print(\"\\n================= Summary: CO-TEACHING only (NUM_RUNS={}, epochs={}) =========\".format(NUM_RUNS, EPOCHS))\n",
    "for (ds, mk, m, s) in results_coteach:\n",
    "    print(f\"{ds:<14} {mk:<12} mean={m:.4f}  std={s:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5328_A1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
