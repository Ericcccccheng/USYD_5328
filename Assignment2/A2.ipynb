{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66e5557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np, random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca8ca69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(NpzFile 'datasets/FashionMNIST0.6.npz' with keys: Xtr, Str, Xts, Yts)\n",
      "Training data shape: (18000, 784)\n",
      "Training labels shape: (18000,)\n",
      "Test data shape: (3000, 784)\n",
      "Test labels shape: (3000,)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = 'datasets/FashionMNIST0.3.npz'\n",
    "dataset2 = 'datasets/FashionMNIST0.6.npz'\n",
    "dataset3 = 'datasets/CIFAR.npz'\n",
    "\n",
    "data = np.load(dataset2)\n",
    "\n",
    "print(data.keys())\n",
    "\n",
    "Xtr = data['Xtr']\n",
    "Str = data['Str']\n",
    "Xts = data['Xts']\n",
    "Yts = data['Yts']\n",
    "\n",
    "print(\"Training data shape:\", Xtr.shape)\n",
    "print(\"Training labels shape:\", Str.shape)\n",
    "print(\"Test data shape:\", Xts.shape)\n",
    "print(\"Test labels shape:\", Yts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_T_by_repeats(X, s, n_classes=3, R=10, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    T_counts = np.zeros((n_classes, n_classes), dtype=float)\n",
    "    for r in range(R):\n",
    "        X_tr, X_va, s_tr, s_va = train_test_split(\n",
    "            X, s, test_size=0.2, stratify=s, random_state=seed + r\n",
    "        )\n",
    "        clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=seed + r)\n",
    "        y_hat = clf.fit(X_tr, s_tr).predict(X_va)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            mask = (y_hat == i)\n",
    "            if mask.sum() == 0:\n",
    "                T_counts[i] += 1.0   # 平滑时等价于 +[1,1,1]\n",
    "            else:\n",
    "                T_counts[i] += np.bincount(s_va[mask], minlength=n_classes)\n",
    "\n",
    "    # 轻度平滑，避免 0：可改成 +1 更强\n",
    "    T_counts += 1e-6\n",
    "    T_hat = T_counts / T_counts.sum(axis=1, keepdims=True)\n",
    "    return T_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b68a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8 0.2 0. ]\n",
      " [0.  0.8 0.2]\n",
      " [0.2 0.  0.8]]\n"
     ]
    }
   ],
   "source": [
    "data = np.load(dataset1)\n",
    "X = data['Xtr'].reshape(len(data['Xtr']), -1)\n",
    "s = data['Str']\n",
    "\n",
    "seed = 10086\n",
    "np.random.seed(seed); random.seed(seed); os.environ[\"PYTHONHASHSEED\"]=str(seed)\n",
    "\n",
    "T_hat = estimate_T_by_repeats(X, s, n_classes=3, R=10, seed=seed)\n",
    "print(np.round(T_hat, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43aace3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m     26\u001b[39m     mask = (pre_labels == i)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     cnt = np.bincount(\u001b[43mS_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m, minlength=\u001b[32m3\u001b[39m).astype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m     28\u001b[39m     T[i] = cnt / cnt.sum()\n\u001b[32m     31\u001b[39m T = T / T.sum(axis=\u001b[32m1\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "data = np.load(dataset2)\n",
    "X_tr1 = data['Xtr']\n",
    "S_tr1 = data['Str']\n",
    "\n",
    "X_tr1_flatten = X_tr1.reshape((X_tr1.shape[0], -1))\n",
    "\n",
    "X_train, X_val, S_train, S_val = train_test_split(X_tr1_flatten, S_tr1, test_size=0.2, random_state=42)\n",
    "\n",
    "n_classes = 3\n",
    "T = np.zeros((n_classes, n_classes))\n",
    "\n",
    "seed = 10086\n",
    "np.random.seed(seed); random.seed(seed); os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "#model = LogisticRegression(max_iter=500) 0.6\n",
    "#model = RandomForestClassifier() 0.7   \n",
    "#model = make_pipeline(StandardScaler(),SVC()) 0.7\n",
    "model = RandomForestClassifier(n_estimators=500, random_state=seed, n_jobs=-1)\n",
    "X_train, X_val, S_train, S_val = train_test_split(\n",
    "    X_tr1_flatten, S_tr1, test_size=0.2, stratify=S_tr1, random_state=seed\n",
    ")\n",
    "\n",
    "pre_labels = model.fit(X_train, S_train).predict_proba(X_val)\n",
    "\n",
    "for i in range(3):\n",
    "    mask = (pre_labels == i)\n",
    "    cnt = np.bincount(S_val[mask], minlength=3).astype(float)\n",
    "    T[i] = cnt / cnt.sum()\n",
    "    \n",
    "\n",
    "T = T / T.sum(axis=1, keepdims=True)\n",
    "\n",
    "print(np.round(T, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96b564f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.3 0.2]\n",
      " [0.3 0.5 0.2]\n",
      " [0.3 0.3 0.5]]\n"
     ]
    }
   ],
   "source": [
    "data = np.load(dataset3)\n",
    "X_tr1 = data['Xtr']\n",
    "S_tr1 = data['Str']\n",
    "\n",
    "X_tr1_flatten = X_tr1.reshape((X_tr1.shape[0], -1))\n",
    "\n",
    "X_train, X_val, S_train, S_val = train_test_split(X_tr1_flatten, S_tr1, test_size=0.2, random_state=42)\n",
    "\n",
    "n_classes = 3\n",
    "T = np.zeros((n_classes, n_classes))\n",
    "\n",
    "seed = 10086\n",
    "np.random.seed(seed); random.seed(seed); os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "#model = LogisticRegression(max_iter=500) 0.6\n",
    "#model = RandomForestClassifier() 0.7   \n",
    "#model = make_pipeline(StandardScaler(),SVC()) 0.7\n",
    "model = ExtraTreesClassifier(n_estimators=500, random_state=seed, n_jobs=-1)\n",
    "X_train, X_val, S_train, S_val = train_test_split(\n",
    "    X_tr1_flatten, S_tr1, test_size=0.2, stratify=S_tr1, random_state=seed\n",
    ")\n",
    "\n",
    "pre_labels = model.fit(X_train, S_train).predict_proba(X_val)\n",
    "\n",
    "for i in range(3):\n",
    "    idx = np.argsort(pre_labels[:, i])[-max(1, int(0.03*len(S_val))):]\n",
    "    T[i] = np.mean(pre_labels[idx], axis=0)\n",
    "\n",
    "T = T / T.sum(axis=1, keepdims=True)\n",
    "\n",
    "print(np.round(T, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23238a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3 0.4 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n"
     ]
    }
   ],
   "source": [
    "data2 = np.load(dataset2)\n",
    "X_tr2 = data2['Xtr']\n",
    "S_tr2 = data2['Str']\n",
    "\n",
    "X_tr2_flatten = X_tr2.reshape((X_tr2.shape[0], -1))\n",
    "\n",
    "X_train, X_val, S_train, S_val = train_test_split(X_tr2_flatten, S_tr2, test_size=0.2, random_state=42)\n",
    "\n",
    "n_classes = 3\n",
    "T2 = np.zeros((n_classes, n_classes))\n",
    "\n",
    "#model = LogisticRegression(max_iter=500) 0.6\n",
    "#model = RandomForestClassifier() 0.7\n",
    "#model = make_pipeline(StandardScaler(),SVC()) 0.7\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train, S_train)\n",
    "\n",
    "pre_labels = model.predict(X_val)\n",
    "\n",
    "for i in range(len(S_val)):\n",
    "    noi_label = S_val[i]\n",
    "    pre_label = pre_labels[i]\n",
    "    T2[pre_label, noi_label] += 1\n",
    "\n",
    "T2 = T2 / T2.sum(axis=1, keepdims=True)\n",
    "\n",
    "T2_orig = T2.copy()\n",
    "T2 = np.round(T2, 1)\n",
    "\n",
    "row_sums = T2.sum(axis=1)\n",
    "rows_to_fix = np.where(np.isclose(row_sums, 0.9))[0]\n",
    "\n",
    "for r in rows_to_fix:\n",
    "    c = int(np.argmax(T2_orig[r]))\n",
    "    T2[r, c] = np.round(T2[r, c] + 0.1, 1)\n",
    "\n",
    "print(T2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e5b2c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3 0.4 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n"
     ]
    }
   ],
   "source": [
    "data3 = np.load(dataset3)\n",
    "X_tr3 = data3['Xtr']\n",
    "S_tr3 = data3['Str']\n",
    "\n",
    "X_tr3_flatten = X_tr3.reshape((X_tr3.shape[0], -1))\n",
    "\n",
    "X_train, X_val, S_train, S_val = train_test_split(X_tr3_flatten, S_tr3, test_size=0.2, random_state=42)\n",
    "\n",
    "n_classes = 3\n",
    "T3 = np.zeros((n_classes, n_classes))\n",
    "\n",
    "#model = LogisticRegression(max_iter=500) 0.6\n",
    "#model = RandomForestClassifier() 0.7\n",
    "#model = make_pipeline(StandardScaler(),SVC()) 0.7\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train, S_train)\n",
    "\n",
    "pre_labels = model.predict(X_val)\n",
    "\n",
    "for i in range(len(S_val)):\n",
    "    noi_label = S_val[i]\n",
    "    pre_label = pre_labels[i]\n",
    "    T3[pre_label, noi_label] += 1\n",
    "\n",
    "T3 = T3 / T3.sum(axis=1, keepdims=True)\n",
    "\n",
    "T3_orig = T3.copy()\n",
    "T3 = np.round(T3, 1)\n",
    "\n",
    "row_sums = T3.sum(axis=1)\n",
    "rows_to_fix = np.where(np.isclose(row_sums, 0.9))[0]\n",
    "\n",
    "for r in rows_to_fix:\n",
    "    c = int(np.argmax(T3_orig[r]))\n",
    "    T3[r, c] = np.round(T3[r, c] + 0.1, 1)\n",
    "\n",
    "print(T3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5328_A1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
